{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 07\n",
    "## Robert Knox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import pandas\n",
    "import numpy\n",
    "import optparse\n",
    "\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pandas.read_csv(\"dev-access.csv\", engine='python', quotechar='|', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataframe.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26773, 2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>26773.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.499010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.500008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  1\n",
       "count  26773.000000\n",
       "mean       0.499010\n",
       "std        0.500008\n",
       "min        0.000000\n",
       "25%        0.000000\n",
       "50%        0.000000\n",
       "75%        1.000000\n",
       "max        1.000000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"timestamp\":1502738402858,\"method\":\"post\",\"query\":{},\"path\":\"/login\",\"statusCode\":401,\"source\":{\"remoteAddress\":\"135.83.221.190\"},\"route\":\"/login\",\"headers\":{\"host\":\"localhost:8002\",\"connection\":\"keep-alive\",\"cache-control\":\"no-cache\",\"accept\":\"*/*\",\"accept-encoding\":\"gzip, deflate, br\",\"accept-language\":\"en-US,en;q=0.8,es;q=0.6\",\"content-type\":\"application/json\",\"content-length\":\"45\"},\"requestPayload\":{\"username\":\"33Michele\",\"password\":\"lelgoec\"},\"responsePayload\":{\"statusCode\":401,\"error\":\"Unauthorized\",\"message\":\"Invalid Login\"}}\n"
     ]
    }
   ],
   "source": [
    "print(dataframe[0][10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = dataset[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, item in enumerate(X):\n",
    "    # Quick hack to space out json elements\n",
    "    reqJson = json.loads(item, object_pairs_hook=OrderedDict)\n",
    "    del reqJson['timestamp']\n",
    "    del reqJson['headers']\n",
    "    del reqJson['source']\n",
    "    del reqJson['route']\n",
    "    del reqJson['responsePayload']\n",
    "    X[index] = json.dumps(reqJson, separators=(',', ':'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"method\":\"post\",\"query\":{},\"path\":\"/login\",\"statusCode\":401,\"requestPayload\":{\"username\":\"Carl2\",\"password\":\"bo\"}}\n"
     ]
    }
   ],
   "source": [
    "print(X[0])\n",
    "tokenizer = Tokenizer(filters='\\t\\n', char_level=True)\n",
    "tokenizer.fit_on_texts(X)\n",
    "\n",
    "# we will need this later\n",
    "num_words = len(tokenizer.word_index)+1\n",
    "X = tokenizer.texts_to_sequences(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X is now broken down into the relevant portions of the log which is then tokenized to be the char value for each character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_log_length = 1024\n",
    "X_processed = sequence.pad_sequences(X, maxlen=max_log_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[18,\n",
       " 1,\n",
       " 20,\n",
       " 2,\n",
       " 3,\n",
       " 14,\n",
       " 7,\n",
       " 11,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 13,\n",
       " 7,\n",
       " 6,\n",
       " 3,\n",
       " 1,\n",
       " 10,\n",
       " 1,\n",
       " 16,\n",
       " 8,\n",
       " 2,\n",
       " 9,\n",
       " 15,\n",
       " 1,\n",
       " 4,\n",
       " 18,\n",
       " 19,\n",
       " 10,\n",
       " 1,\n",
       " 13,\n",
       " 5,\n",
       " 3,\n",
       " 14,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 25,\n",
       " 12,\n",
       " 7,\n",
       " 26,\n",
       " 24,\n",
       " 21,\n",
       " 1,\n",
       " 10,\n",
       " 1,\n",
       " 6,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 8,\n",
       " 6,\n",
       " 17,\n",
       " 7,\n",
       " 11,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 23,\n",
       " 22,\n",
       " 29,\n",
       " 10,\n",
       " 1,\n",
       " 9,\n",
       " 2,\n",
       " 16,\n",
       " 8,\n",
       " 2,\n",
       " 6,\n",
       " 3,\n",
       " 13,\n",
       " 5,\n",
       " 15,\n",
       " 12,\n",
       " 7,\n",
       " 5,\n",
       " 11,\n",
       " 1,\n",
       " 4,\n",
       " 18,\n",
       " 1,\n",
       " 8,\n",
       " 6,\n",
       " 2,\n",
       " 9,\n",
       " 21,\n",
       " 5,\n",
       " 20,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 17,\n",
       " 5,\n",
       " 9,\n",
       " 12,\n",
       " 28,\n",
       " 1,\n",
       " 10,\n",
       " 1,\n",
       " 13,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 32,\n",
       " 7,\n",
       " 9,\n",
       " 11,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 40,\n",
       " 7,\n",
       " 1,\n",
       " 19,\n",
       " 19]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_processed, Y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1 - RNN\n",
    "\n",
    "embedding layer, LSTM layer, and Dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 1024, 32)          2016      \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 64)                24832     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 26,913\n",
      "Trainable params: 26,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "m1 = Sequential()\n",
    "m1.add(Embedding(input_dim=num_words, output_dim = 32, input_length  = max_log_length))\n",
    "m1.add(LSTM(units = 64, recurrent_dropout=0.5))\n",
    "m1.add(Dense(units = 1, activation = 'relu'))\n",
    "\n",
    "m1.compile(loss = 'binary_crossentropy',\n",
    "          optimizer = 'adam',\n",
    "          metrics = ['accuracy'])\n",
    "\n",
    "m1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15059 samples, validate on 5020 samples\n",
      "Epoch 1/3\n",
      "15059/15059 [==============================] - 222s 15ms/step - loss: 0.8689 - acc: 0.5093 - val_loss: 0.6508 - val_acc: 0.6375\n",
      "Epoch 2/3\n",
      "15059/15059 [==============================] - 227s 15ms/step - loss: 0.6164 - acc: 0.6029 - val_loss: 0.5740 - val_acc: 0.6034\n",
      "Epoch 3/3\n",
      "15059/15059 [==============================] - 248s 16ms/step - loss: 0.6028 - acc: 0.5918 - val_loss: 0.4359 - val_acc: 0.5631\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23db0e409b0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Use a validation split of 0.25, epochs=3 and batch size = 128\n",
    "m1.fit(X_train,y_train,batch_size=128,epochs = 3, validation_split=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6694/6694 [==============================] - 31s 5ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.44348750613609783, 0.5751419181356439]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = m1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = m1.predict(X_test)\n",
    "yhat = [0 if i==0 else 1 for i in yhat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = numpy.array(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, ..., 0, 0, 1])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_int = y_test.astype(int)\n",
    "y_test_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.03      0.05      3423\n",
      "           1       0.50      1.00      0.66      3271\n",
      "\n",
      "   micro avg       0.50      0.50      0.50      6694\n",
      "   macro avg       0.75      0.51      0.36      6694\n",
      "weighted avg       0.75      0.50      0.35      6694\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n",
    "\n",
    "cr = classification_report(y_test_int,yhat)\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 1024, 32)          2016      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024, 32)          0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 64)                24832     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 26,913\n",
      "Trainable params: 26,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "m2 = Sequential()\n",
    "m2.add(Embedding(input_dim=num_words, output_dim = 32, input_length  = max_log_length))\n",
    "m2.add(Dropout(rate = 0.5))                 \n",
    "m2.add(LSTM(units = 64, recurrent_dropout=0.5))\n",
    "m2.add(Dropout(rate = 0.5))\n",
    "m2.add(Dense(units = 1, activation = 'sigmoid'))\n",
    "\n",
    "m2.compile(loss = 'binary_crossentropy',\n",
    "          optimizer = 'adam',\n",
    "          metrics = ['accuracy'])\n",
    "\n",
    "m2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15059 samples, validate on 5020 samples\n",
      "Epoch 1/3\n",
      "15059/15059 [==============================] - 243s 16ms/step - loss: 0.5993 - acc: 0.6645 - val_loss: 0.3248 - val_acc: 0.8681\n",
      "Epoch 2/3\n",
      "15059/15059 [==============================] - 253s 17ms/step - loss: 0.3720 - acc: 0.8643 - val_loss: 0.2834 - val_acc: 0.9488\n",
      "Epoch 3/3\n",
      "15059/15059 [==============================] - 259s 17ms/step - loss: 0.2560 - acc: 0.9266 - val_loss: 0.1616 - val_acc: 0.9550\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23dea280e48>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Use a validation split of 0.25, epochs=3 and batch size = 128\n",
    "m2.fit(X_train,y_train,batch_size=128,epochs = 3, validation_split=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6694/6694 [==============================] - 32s 5ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.16965968555052635, 0.9530923214819241]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat2 = m2.predict(X_test)\n",
    "yhat2 = [1 if i>=0.5 else 0 for i in yhat2]\n",
    "yhat2 = numpy.array(yhat2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.99      0.96      3423\n",
      "           1       0.99      0.91      0.95      3271\n",
      "\n",
      "   micro avg       0.95      0.95      0.95      6694\n",
      "   macro avg       0.96      0.95      0.95      6694\n",
      "weighted avg       0.96      0.95      0.95      6694\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cr2 = classification_report(y_test_int,yhat2)\n",
    "print(cr2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, 1024, 32)          2016      \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 1024, 32)          0         \n",
      "_________________________________________________________________\n",
      "lstm_11 (LSTM)               (None, 64)                24832     \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 35,297\n",
      "Trainable params: 35,297\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "m3 = Sequential()\n",
    "m3.add(Embedding(input_dim=num_words, output_dim = 32, input_length  = max_log_length))\n",
    "m3.add(Dropout(rate = 0.5))                 \n",
    "m3.add(LSTM(units = 64, recurrent_dropout=0.5))\n",
    "m3.add(Dropout(rate = 0.5))\n",
    "m3.add(Dense(units = 128, activation = 'relu'))\n",
    "m3.add(Dropout(rate = 0.5))\n",
    "m3.add(Dense(units = 1, activation = 'sigmoid'))\n",
    "\n",
    "m3.compile(loss = 'binary_crossentropy',\n",
    "          optimizer = 'Adadelta',\n",
    "          metrics = ['accuracy'])\n",
    "\n",
    "m3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15059 samples, validate on 5020 samples\n",
      "Epoch 1/3\n",
      "15059/15059 [==============================] - 413s 27ms/step - loss: 0.6907 - acc: 0.5283 - val_loss: 0.6797 - val_acc: 0.5673\n",
      "Epoch 2/3\n",
      "15059/15059 [==============================] - 407s 27ms/step - loss: 0.6302 - acc: 0.6164 - val_loss: 0.5450 - val_acc: 0.6566\n",
      "Epoch 3/3\n",
      "15059/15059 [==============================] - 401s 27ms/step - loss: 0.5321 - acc: 0.7255 - val_loss: 0.3216 - val_acc: 0.9157\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23dbab5cda0>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m3.fit(X_train,y_train,batch_size=128,epochs = 3, validation_split=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6694/6694 [==============================] - 49s 7ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.32928738306984107, 0.9141021810576636]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m3.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat3 = m3.predict(X_test)\n",
    "yhat3 = [1 if i>=0.5 else 0 for i in yhat3]\n",
    "yhat3 = numpy.array(yhat3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.99      0.92      3423\n",
      "           1       0.99      0.84      0.90      3271\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      6694\n",
      "   macro avg       0.92      0.91      0.91      6694\n",
      "weighted avg       0.92      0.91      0.91      6694\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cr3 = classification_report(y_test_int,yhat3)\n",
    "print(cr3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Explain the difference between the relu activation function and the sigmoid activation function.\n",
    "\n",
    "Activation functions have several properties that must be considered for their use in a neural network. These include:\n",
    "\n",
    "* Differentiablity - If the activation function is differentiable for all real numbers it simplifies the process of back propagation\n",
    "\n",
    "* Computational Efficiency - The activation function must be calculated for each neuron in each epoch so it is desirable to  have a computationallly efficient function for the sake of expediency.\n",
    "\n",
    "* Symmetry around the origin - Functions that are symmetric around zero tend to perform better as the neuron has a less limited output.\n",
    "\n",
    "* Vanishing & Exploding Gradients - As more derivatives are calculated the value of gradient tends to either diminish to zero producing a result that does not get close to a local minimum of the cost function or the gradient may explode and move away from the local minimum.\n",
    "\n",
    "The relu activation function is non-differentiable at zero and non-symmetric around the origin but is preferred in deep neural networks because it is computationally efficient & helps prevent vanishing gradients.\n",
    "\n",
    "The sigmoid activation function is differentiable for all $\\mathbb{R}$ and is easily differentiable. However, since it only produces results between (0,1) it reduces the input space drastically. This produces a vanishing gradient making it unsuitable for deep neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In regards to question 5, which of these activation functions performed the best (they were used in Model 1 & Model 2) ? Why do you think that is?\n",
    "\n",
    "The sigmoid activation function performed better than relu in this assignment. I believe it worked better as the final goal of the model was to predict a 0 or 1 for each record. The sigmoid function is ideally suited for this as its output range is (0,1) whereas relu produces outputs (0,$\\infty$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain how dropout works (you can look at the keras code) for (a) training, and (b) test data sets.\n",
    "\n",
    "In training, the dropout creates a noise shape that is a binary mask the input tensor. The probability that a neuron will be masked is based on the rate. This mask prevents the neuron from being considered for the feed forward calculation as well as preventing updates in the backpropagation step.\n",
    "\n",
    "In test, there is no dropout of neurons; the dropout just affects the neurons in training to make the model have a more robust generalization error. In testing we simply let the network make its decision based on the training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain why problems such as this are better modeled with RNNs than CNNs.\n",
    "\n",
    "RNNs allow us to retain memory from previous states in the data are not IID. A CNN would let us take a highly dimensional feature space and reduce it down to smaller elements but the assumption is that the data are IID."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain what RNN problem is solved using LSTM and briefly describe how.\n",
    "\n",
    "RNNs suffer from problems when the relevant information is not from a state that directly precedes the current state. LSTM solves this issue by retaining memory across all states to account for long term dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
